{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './PythonFiles/')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast\n",
    "import unicodedata\n",
    "import math\n",
    "import warnings\n",
    "import pickle\n",
    "from CategoryChanger import CategoryChanger\n",
    "from Classifiers import Classifiers\n",
    "from DFPreProcessing import CleanDF, DataPreProcessor\n",
    "from sklearn.metrics import *\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_prediction(test):\n",
    "    prediction = []\n",
    "    for i in range(len(test)):\n",
    "        num = random.randint(0,1)\n",
    "        if num == 0:\n",
    "            prediction.append('NScam')\n",
    "        else:\n",
    "            prediction.append('Scam')\n",
    "    return prediction\n",
    "\n",
    "def all_scores(Test_Y, prediction):\n",
    "    labels = [\"NScam\",\"Scam\"]\n",
    "    scores = {\"precision\":{\"NScam\":0,\"Scam\":0},\"recall\":{\"NScam\":0,\"Scam\":0},\"f1-score\":{\"NScam\":0,\"Scam\":0},\"accuracy\":0}\n",
    "\n",
    "    for label in labels:\n",
    "        scores[\"precision\"][label] = precision_score(Test_Y, prediction, pos_label=label)\n",
    "        scores[\"recall\"][label] = recall_score(Test_Y, prediction, pos_label=label)\n",
    "        scores[\"f1-score\"][label] = f1_score(Test_Y, prediction, pos_label=label)\n",
    "    \n",
    "    scores[\"accuracy\"] = accuracy_score(Test_Y, prediction)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def get_avg(scores):\n",
    "    avg_scores = scores[0]\n",
    "\n",
    "    for score in scores[1:]:\n",
    "        for key, value in score.items():\n",
    "            if isinstance(value, dict):\n",
    "                for innerKey, innerValue in value.items():\n",
    "                    avg_scores[key][innerKey] += innerValue\n",
    "            else:\n",
    "                avg_scores[key] += value\n",
    "\n",
    "    for key, value in avg_scores.items():\n",
    "        if isinstance(value, dict):\n",
    "            for innerKey, innerValue in value.items():\n",
    "                avg_scores[key][innerKey] = round(innerValue/len(scores), 2)\n",
    "        else:\n",
    "            avg_scores[key] = round(value/len(scores), 2)\n",
    "        \n",
    "    return avg_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_state(val):\n",
    "    if int(val) > 2017:\n",
    "        return \"Scam\"\n",
    "    elif int(val) < 2018:\n",
    "        return \"NScam\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_be_classified = [['text_final','Location','Image_Counts']]\n",
    "\n",
    "trainData = pd.read_csv('trainData.csv', low_memory=False)\n",
    "testData = pd.read_csv('testData.csv', low_memory=False)\n",
    "\n",
    "trainData = CleanDF(trainData).getConvertedDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx,row in enumerate(trainData['Join_date']):\n",
    "    trainData.loc[indx,'label'] = str(find_value_state(trainData['Join_date'][indx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataPreProcessor().processData(testData.copy(deep=True))\n",
    "Test_X = DataPreProcessor().get_feature_datasets(test, features_to_be_classified)[0]\n",
    "Test_Y = testData['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = Classifiers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_cnt = 1000\n",
    "scores = []\n",
    "\n",
    "for i in range(loop_cnt):\n",
    "    prediction = random_prediction(Test_X)\n",
    "    scores.append(all_scores(Test_Y,prediction))\n",
    "\n",
    "avg_scores = get_avg(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': {'NScam': 0.52, 'Scam': 0.48},\n",
       " 'recall': {'NScam': 0.5, 'Scam': 0.5},\n",
       " 'f1-score': {'NScam': 0.51, 'Scam': 0.49},\n",
       " 'accuracy': 0.5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
